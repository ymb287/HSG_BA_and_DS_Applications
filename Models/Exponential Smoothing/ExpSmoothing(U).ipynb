{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0LKQFiTuOFkO",
   "metadata": {
    "id": "0LKQFiTuOFkO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tsIbJyJEODY5",
   "metadata": {
    "id": "tsIbJyJEODY5"
   },
   "source": [
    "#Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fae9a4-1783-4005-a8a0-fec7cbc3b30d",
   "metadata": {
    "id": "43fae9a4-1783-4005-a8a0-fec7cbc3b30d"
   },
   "outputs": [],
   "source": [
    "# Read csv\n",
    "df = pd.read_csv('~/Desktop/DATA_PROJECT/HSG_BA_and_DS_Applications/data/processed/final_df.csv', parse_dates=True, index_col=0)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df_copy =df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t7Eldp-dPdzt",
   "metadata": {
    "id": "t7Eldp-dPdzt"
   },
   "source": [
    "#Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4eb53-3a66-4fe6-8327-0b46807e4413",
   "metadata": {
    "id": "27f4eb53-3a66-4fe6-8327-0b46807e4413"
   },
   "outputs": [],
   "source": [
    "# Example for one location\n",
    "location = 'Little Collins St-Swanston St (East)'\n",
    "\n",
    "# Prepare the data for Prophet (from the copied dataset)\n",
    "data = df_copy[['Hour', location, 'IsPublicHoliday', 'temp', 'humidity', 'rain_1h', 'clouds_all']].rename(\n",
    "    columns={'Hour': 'ds', location: 'y'}\n",
    ")\n",
    "\n",
    "# Convert 'ds' to datetime format\n",
    "data['ds'] = pd.to_datetime(data['ds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mduTyweVDFct",
   "metadata": {
    "id": "mduTyweVDFct"
   },
   "source": [
    "#Evaluation metrics and grid search Hyperparameters training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DuJcciEvCvCF",
   "metadata": {
    "id": "DuJcciEvCvCF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pCmDrwvnDZJ1",
   "metadata": {
    "id": "pCmDrwvnDZJ1"
   },
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    y_true_nonzero = np.where(y_true == 0, np.nan, y_true)  # Avoid divide-by-zero for MAPE\n",
    "    mape = (np.abs((y_true - y_pred) / y_true_nonzero)).mean() * 100  # Exclude NaNs\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, mape, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PSklEpLLDbR_",
   "metadata": {
    "id": "PSklEpLLDbR_"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('~/Desktop/DATA_PROJECT/HSG_BA_and_DS_Applications/data/processed/final_df.csv', parse_dates=True, index_col=0)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OmrIdfCXDigW",
   "metadata": {
    "id": "OmrIdfCXDigW"
   },
   "outputs": [],
   "source": [
    "# Locations to process\n",
    "locations = [\n",
    "    'Little Collins St-Swanston St (East)',\n",
    "    'Faraday St-Lygon St (West)',\n",
    "    'Melbourne Central',\n",
    "    'Chinatown-Lt Bourke St (South)',\n",
    "    'Lonsdale St (South)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E3MI6ACfDkPL",
   "metadata": {
    "id": "E3MI6ACfDkPL"
   },
   "outputs": [],
   "source": [
    "results = {}  # To store evaluation metrics\n",
    "best_params = {}  # To store the best hyperparameters for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Sb4A7Bf7DmGN",
   "metadata": {
    "id": "Sb4A7Bf7DmGN"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for Exponential Smoothing\n",
    "trend_options = [None, 'add', 'mul']  # Trend component: None, additive, or multiplicative\n",
    "seasonal_options = [None, 'add', 'mul']  # Seasonal component: None, additive, or multiplicative\n",
    "seasonal_periods = [24, 7 * 24]  # Hourly (daily) and weekly seasonalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fz2sUJ6DtEW",
   "metadata": {
    "id": "7fz2sUJ6DtEW"
   },
   "outputs": [],
   "source": [
    "# Process each location\n",
    "for location in locations:\n",
    "    print(f\"Processing {location}...\")\n",
    "\n",
    "    # Prepare data for the current location\n",
    "    data = df_copy[['Hour', location]].rename(columns={'Hour': 'ds', location: 'y'})\n",
    "    data['ds'] = pd.to_datetime(data['ds'])\n",
    "    data = data[data['y'] > 0]  # Remove zero counts for stability\n",
    "\n",
    "    # Split into training and testing (last 16 days for testing)\n",
    "    split_index = int(len(data) - 16 * 24)\n",
    "    train_data = data.iloc[:split_index]\n",
    "    test_data = data.iloc[split_index:]\n",
    "\n",
    "    # Extract target variable\n",
    "    y_train = train_data['y']\n",
    "    y_test = test_data['y']\n",
    "\n",
    "    # Initialize variables to track the best parameters\n",
    "    best_rmse = float('inf')\n",
    "    best_model = None\n",
    "    best_params_location = None\n",
    "\n",
    "    # Grid search over hyperparameters\n",
    "    for trend in trend_options:\n",
    "        for seasonal in seasonal_options:\n",
    "            for seasonal_period in seasonal_periods:\n",
    "                try:\n",
    "                    # Initialize and fit the Exponential Smoothing model\n",
    "                    model = ExponentialSmoothing(\n",
    "                        y_train,\n",
    "                        trend=trend,\n",
    "                        seasonal=seasonal,\n",
    "                        seasonal_periods=seasonal_period,\n",
    "                        initialization_method='estimated'\n",
    "                    )\n",
    "                    fitted_model = model.fit()\n",
    "\n",
    "                    # Forecast on the test set\n",
    "                    y_pred = fitted_model.forecast(steps=len(y_test))\n",
    "\n",
    "                    # Evaluate performance\n",
    "                    rmse, mape, r2 = calculate_metrics(y_test.values, y_pred)\n",
    "\n",
    "                    # Update the best model if the current one is better\n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse = rmse\n",
    "                        best_model = fitted_model\n",
    "                        best_params_location = (trend, seasonal, seasonal_period)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with parameters (trend={trend}, seasonal={seasonal}, seasonal_period={seasonal_period}): {e}\")\n",
    "\n",
    "    # Save the best model and parameters\n",
    "    model_filename = f'ets_model_{location.replace(\" \", \"_\").replace(\"–\", \"_\")}.pkl'\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    best_params[location] = best_params_location\n",
    "\n",
    "    # Forecast on the test set using the best model\n",
    "    y_pred = best_model.forecast(steps=len(y_test))\n",
    "\n",
    "    # Evaluate final performance\n",
    "    rmse, mape, r2 = calculate_metrics(y_test.values, y_pred)\n",
    "\n",
    "    # Store the metrics\n",
    "    results[location] = {'RMSE': rmse, 'MAPE': mape, 'R²': r2}\n",
    "    print(f\"Best parameters for {location}: {best_params_location}\")\n",
    "    print(f\"Metrics for {location}: RMSE={rmse}, MAPE={mape}, R²={r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e0d13-db8e-4420-b435-43e8a73e4ecc",
   "metadata": {
    "id": "db5e0d13-db8e-4420-b435-43e8a73e4ecc",
    "outputId": "e713576f-ba0d-4af2-a2dd-8fa303df897a"
   },
   "outputs": [],
   "source": [
    "# Save the results and best parameters to CSV files\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "results_df.to_csv('ets_evaluation_metrics.csv', index=True)\n",
    "print(\"Evaluation metrics saved to 'ets_evaluation_metrics.csv'\")\n",
    "\n",
    "best_params_df = pd.DataFrame.from_dict(best_params, orient='index', columns=['Trend', 'Seasonal', 'Seasonal_Period'])\n",
    "best_params_df.to_csv('ets_best_hyperparameters.csv', index=True)\n",
    "print(\"Best hyperparameters saved to 'ets_best_hyperparameters.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WXJBz3EADzhe",
   "metadata": {
    "id": "WXJBz3EADzhe"
   },
   "source": [
    "#Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-uzX3VjJD32G",
   "metadata": {
    "id": "-uzX3VjJD32G"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rUc_GkBzD65A",
   "metadata": {
    "id": "rUc_GkBzD65A"
   },
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store predictions\n",
    "predictions_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SRZSKPYJEAlq",
   "metadata": {
    "id": "SRZSKPYJEAlq"
   },
   "outputs": [],
   "source": [
    "# Loop through each location to generate predictions and save them\n",
    "for location in locations:\n",
    "    print(f\"Processing {location}...\")\n",
    "\n",
    "    # Prepare data for the current location\n",
    "    data = df_copy[['Hour', location]].rename(columns={'Hour': 'ds', location: 'y'})\n",
    "    data['ds'] = pd.to_datetime(data['ds'])\n",
    "    data = data[data['y'] > 0]  # Remove zero counts for stability\n",
    "\n",
    "    # Split into training and testing (last 16 days for testing)\n",
    "    split_index = int(len(data) - 16 * 24)\n",
    "    train_data = data.iloc[:split_index]\n",
    "    test_data = data.iloc[split_index:]\n",
    "\n",
    "    # Extract target variable\n",
    "    y_test = test_data['y']\n",
    "\n",
    "    # Forecast on the test set using the best model\n",
    "    y_pred = best_model.forecast(steps=len(y_test))\n",
    "\n",
    "    # Add predictions and actual values to the DataFrame\n",
    "    predictions_df[f\"{location}_predicted\"] = y_pred\n",
    "    predictions_df[f\"{location}_actual\"] = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DqahpH7gEEZd",
   "metadata": {
    "id": "DqahpH7gEEZd"
   },
   "outputs": [],
   "source": [
    "# Add a timestamp index from the test data\n",
    "predictions_df.index = test_data['ds'].values  # Assuming 'ds' is the timestamp column in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1192ea-6d64-40f0-8d37-381c2a77c971",
   "metadata": {
    "id": "6a1192ea-6d64-40f0-8d37-381c2a77c971",
    "outputId": "9c43f4fc-f188-424a-e16c-4a584d79383f"
   },
   "outputs": [],
   "source": [
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('ets_predictions.csv', index=True)\n",
    "print(\"Predictions saved to 'ets_predictions.csv'\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
